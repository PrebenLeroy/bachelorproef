%%=============================================================================
%% Experiment
%%=============================================================================

\chapter{Experiment}
\label{ch:experiment}

In dit hoofstuk zal het complete experiment besproken worden. Eerst en vooral zal er gekeken worden naar de mogelijkheid om de algoritmen werkende te hebben. Indien alles goed zal verlopen, zullen de algoritmen uitgevoerd worden op andere datasets. Hierbij wordt dan gekeken of de bekomen resultaten gelijkaardig zijn aan de verkregen resultaten. 

Het gevolg van deze experimenten valt dan te lezen in het volgende hoofdstuk. Er wordt getracht een duidelijke conclusie te vormen die antwoord bied op de vooropgestelde onderzoeksvragen. Het kan natuurlijk ook anders uitdraaien. De algoritmen zouden bijvoorbeeld niet kunnen werken of de resultaten zouden niet kunnen kloppen. Dan zal er moeten gezocht worden naar een duidelijke reden waarom het niet werkt.

\section{WikiSQL}

\subsection{Algoritme}

Het WikiSQL algoritme, welke te verkrijgen is via de Github-repository van \textcite{wikisql}, biedt de gebruiker een geannoteerd semantisch sjabloon voor de ontwikkeling van natuurlijke taalinterfaces. WikiSQL is een dataset, ontworpen door Salesforce en gelijktijdig uitgebracht met hun werk over Seq2SQL. Seq2SQL is reeds besproken in hoofdstuk \ref{ch:stand-van-zaken}, sectie \ref{sec:Salesforce - Seq2SQL}. Dit algoritme wordt later in dit hoofdstuk ook nog kort aangehaald.

Naast Seq2SQL wordt de WikiSQL dataset ook veel gebruikt voor andere algoritmen met doel het natuurlijke taalprogrammering. In hoofdstuk \ref{ch:stand-van-zaken} zijn er reeds een aantal van besproken. Ook wordt bijvoorbeeld SQLNet (sectie \ref{sec:SQLNet}) getraind op basis van de WikiSQL dataset.

\subsection{Opbouw}

De complete Github-repository (\textcite{wikisql}) is opgebouwd uit twee mappen (\textit{lib} en \textit{test}), twee Python-scripts, twee tekstbestanden en een aantal andere bestanden. De lib-map bestaat uit vijf verschillende Python-scripts welke aangeroepen worden tijdens de uitvoering van het algoritme. Zo bevat deze map bijvoorbeeld het script die de data uit de databank kan lezen. 

De volgende map, test, bevat belangrijke scripts en bestanden voor de training en uitvoering van het algoritme. Volgende bestanden kunnen hier teruggevonden worden:
\begin{description}
	\item[Dockerfile] Dit bestand wordt gebruikt om een image te maken voor het trainen en het uitvoeren van het algoritme van WikiSQL via een Docker container. In het bestand zelf worden de commando's beschreven de aanmaak van een voorlopige werkomgeving, waarin het geheel wordt uitgevoerd. Daarnaast zijn de commando's voor de installatie van het algoritme in de Dockerfile terug te vinden, alsook de commando's voor training en uitvoering van het algoritme. Hierover later meer.
	\item[check.py] Dit Python-script zorgt ervoor dat het algoritme getraind wordt. Het gaat op zoek naar data-bestanden in de data-map om daarmee het algoritme te trainen. Indien er een query geen geldig resultaat teruggeeft, stopt het algoritme en geeft het een foutmelding terug. Ook wordt er een foutmelding gegeven indien er geen correcte query kan teruggevonden worden.
	\item[example.pred.dev.jsonl.bz2] Dit is een voorbeeld voorspellingsdocument, gebaseerd op de dev-dataset. Via het bzip2-commando is het mogelijk dit bestand uit te pakken en te kijken naar de eventuele resultaten voor deze set.
\end{description}

In de algemene overzichtspagina van de Github-repository worden nog een aantal andere bestanden vermeld, naast de hierboven beschreven mappen. De belangrijkste bestanden hierbij zijn:
\begin{description}
	\item[data.tar.bz2] Dit is een gearchiveerd bestand. Wanneer dit uitgepakt wordt, wordt er een map \textit{data} aangemaakt met daarin data-bestanden voor drie verschillende sets. Voor elke set wordt er een databank bestand teruggevonden (in SQLite-formaat), een .jsonl bestand met de uitleg over de tabellen alsook een .jsonl bestand met de vraag, query en tabelid. In bijlage \ref{ch:bijlage1} kan een voorbeeld van beide .jsonl bestanden teruggevonden worden.
	\item[evaluate.py] Dit is het script dat uitgevoerd dient te worden nadat het algoritme getraind is. Het script is de uitwerking van het algoritme.
	\item[requirements.txt] Dit document bevat alle, via pip install, te installeren modules om de uitvoering van de Python-scripts mogelijk te maken.
\end{description}

\subsection{Uitvoering}

Om het algoritme lokaal op het systeem te draaien, dient het geheel eerst en vooral geïnstalleerd te worden. Wanneer deze repository is gecloned op het lokale systeem, kan de gebruiker verder gaan met de installatie. De volgende stap is de installatie van de alle vereiste modules. Deze modules staan opgelijst in de \textit{requirements.txt} bestand. Met volgend commando is het mogelijk dit te installeren:
\begin{center}
	\textit{sudo python3.6 -m pip install -r requirements.txt}
\end{center}
Sudo wordt gebruikt opdat de gebruiker direct de correcte rechten bezit om zaken te installeren op het systeem. Door het gebruik van \textit{python3.6 -m} worden de modules enerzijds direct geïnstalleerd voor deze Python-versie, anderzijds helpt Python het zoeken naar deze modules. Er wordt daarnaast ook gebruik gemaakt van de "-r"-parameter. Deze parameter zorgt ervoor dat het \textit{requirements.txt} gelezen wordt en alle vermelde modules geïnstalleerd worden.

De data, nodig voor de uitvoering van de scripts, is terug te vinden in het data.tar.bz2 bestand. Dit bestand dient door de gebruiker uitgepakt te worden. De gebruiker is nu klaar voor de eigenlijke uitvoering van de scripts.

De uitvoering van de eigenlijke scripts kan op twee manieren gebeuren. Enerzijds kunnen de scripts direct uitgevoerd worden in het terminal-venster. Anderzijds kan dit algoritme uitgevoerd worden vanuit een Docker-container.

\subsubsection{Terminal}

Wanneer de scripts uitgevoerd worden direct vanuit de terminal, dient de gebruiker het volgende commando uit de voeren om het bestand waarin voorbeeld voorspellingen vermeld zijn uit te pakken:
\begin{center}
	\textit{sudo apt-get update \&\& sudo apt-get install bzip2}
\end{center}
\begin{center}
	\textit{bzip2 test/example.pred.dev.jsonl.bz2}
\end{center}

Het eerste script dat dient uitgevoerd te worden, \textit{check.py}, zorgt voor de training van het algoritme. Dit script dient uitgevoerd te worden door middel van volgend commando:
\begin{center}
	\textit{sudo python3.6 test/check.py}
\end{center}

De uitvoering van dit script loopt geregeld mis. Het algoritme wordt hier op basis van drie verschillende data-bestanden getraind. Bij het doorlopen van de eerste dataset, \textit{train} - 56355 query's, loopt het een aantal keren mis\dots Maar liefst bij 14 query's meldt het systeem volgende foutmelding: \textit{raise Exception (‘Query {} did not execute to a valid result’.format(query))}. De uitvoering van de query's zouden geen correct resultaat teruggeven. Deze query's werden ter controle ook uitgevoerd in de DB Browser for SQLite. Het betreft hier namelijk SQLite-databanken. Ook in de volgende twee onderzochte datasets, \textit{dev} en \textit{test} liepen er een aantal query's fout tijdens de uitvoering. In tabel \ref{table:wikisqlerrorstrain}, welke te vinden is op het einde dit hoofdstuk, worden de misgelopen query's uit train besproken. In tabellen \ref{table:wikisqlerrorsdev} en \ref{table:wikisqlerrorstest}, terug te vinden op het einde van dit hoofdstuk, worden respectievelijk de misgelopen query's besproken voor de dev-dataset als voor de test-dataset.

Er werd getracht elk van deze query's terug werkende te krijgen, maar dit is niet gelukt. Daarom werd er besloten deze query's weg te laten uit de trainingsdatasets. Er bleven per dataset respectievelijk nog 56341, 8416 en 15875 query's over. 

Wanneer de training van het algoritme afgerond was, dient men ook het evaluatiescript uit te voeren. Dit kan met het volgende commando:
\begin{center}
	\textit{sudo python3.6 evaluate.py data/dev.jsonl data/dev.db test/example.pred.dev.jsonl}
\end{center}

Tijdens deze uitvoering wordt de dev dataset doorlopen om zo een mogelijk resultaat te bekomen. Deze resultaten zullen besproken worden in subsectie \ref{sec:resultwikisql}.

\subsubsection{Docker-container}

De uitvoering via Docker bestaat ook uit twee verschillende stappen. Zoals reeds vermeld in de opbouw van dit algoritme, bevat de map "test" ook een Dockerfile. Dit bestand is zeer belangrijk voor deze manier van werken. Eerst dient men volgend commando uit te voeren om zo de test uit te voeren.
\begin{center}
	\textit{sudo docker build -t wikisqltest -f test/Dockerfile .}
\end{center}

Hier wordt een image gebouwd vanuit de root-map. Via dit commando wordt de uitvoering van het algoritme voorbereid. De echte uitvoering, de uitvoering van de image, gebeurd via volgend commando:
\begin{center}
	\textit{sudo docker run --rm --name wikisqltest wikisqltest}
\end{center}

De bekomen resultaten worden besproken in subsectie \ref{sec:resultwikisql}.

\subsection{Resultaten}
\label{sec:resultwikisql}

Na de uitvoering van het WikiSQL algoritme vanuit de terminal, werden volgende resultaten verkregen:

\{ \\
“ex\_accuracy”: 0.16813212927756654, \\
“lf\_accuracy”: 0.1091967680608365 \\
\}

De uitvoering via de Docker-container toonde volgende resultaten: 

\{ \\
“ex\_accuracy”: 0.5380596128725804, \\
“lf\_accuracy”: 0.35375846099038116 \\
\}

Beide methoden bereiken de opgegeven resultaten niet. De nodige resultaten zijn:

\{ \\
“ex\_accuracy”: 0.37036632039365774, \\
"lf\_accuracy": 0.2334609075997813 \\
\}

Zowel bij de uitvoering van het algoritme via de terminal als de uitvoering via een Docker container mislukten aangezien de training van het algoritme niet compleet is. Bij het uitvoeren in de terminal worden er in totaal, over de drie verschillende test-datasets gezien, 22 query's niet uitgevoerd tijdens het trainen van het algoritme. Bij het uitvoeren van het algoritme door middel van een Docker container, stopt de training hoogstwaarschijnlijk al na 41 procent bij de uitvoering van de train dataset (zoals reeds vermeld bij de uitvoering via terminal). Dit is de reden waarom de vereiste resultaten niet bereikt kunnen worden.  

\section{SQLNet}
\label{sec:sqlnet}

\subsection{Algoritme}

Het SQLNet algoritme (sectie \ref{sec:SQLNet}), welke te verkrijgen is via de Github-repository van \textcite{sqlnet}, biedt de gebruiker een neuraal netwerk aan voor de generatie van query's op basis van natuurlijke taal. Zoals reeds vermeld, wordt het systeem getraind op basis van de WikiSQL dataset. Daarnaast wordt er ook een implementatie gegeven voor het Seq2SQL algoritme voor het voorspellen van SQL-query's.

Het algoritme maakt geen gebruik van reïnforcement learning. Dit is het mechanisme dat ervoor zorgt dat het algoritme een soort beloning krijgt vanaf wanneer er een goede query wordt gegenereerd. Dit in tegenstelling tot Seq2SQL, waar reïnforcement learning weldegelijk wordt toegepast.

\subsection{Opbouw}

De complete Github-repository van SQLNet (\textcite{sqlnet}) is opgebouwd uit twee mappen (namelijk \textit{saved\_model} en sqlnet), drie Python-scripts, één tekstbestand en een aantal andere bestanden. De saved\_model-map bevat één bestand, \textit{.keep}. Dit lege bestand zorgt ervoor dat de map waarin het document zich bevindt beschikbaar blijft bij het publiceren op Github.

De tweede map, sqlnet, bevat ook twee mappen (\textit{lib} en \textit{model}) en twee Python-scripts. De lib-map bevat twee Python-scripts welke aangeroepen worden bij de uitvoering van het algoritme. Deze map bevat het script dat toegang bied tot de databank. De model-map bevat op zijn beurt ook nog een map (\textit{modules}) alsook drie verschillende Python-scripts. Dit geheel aan scripts wordt op zijn beurd ook opgeroepen bij de uitvoering van het algoritme. 

In het overzicht van de Github-repository worden er een aantal andere bestanden opgegeven, naast de hierboven beschreven mappen. De belangrijkste bestanden hierbij zijn:

\begin{description}
	\item[data.tar.bz2] Dit is een gearchiveerd bestand. Nadat het bestand wordt uigepakt, wordt er een map met de naam \textit{data} aangemaakt. Zoals bij WikiSQL, bevat deze map drie verschillende datasets. Per dataset kan er een databank bestand (in SQLite formaat) teruggevonden worden alsook twee .jsonl-bestanden. De .tables.jsonl-bestanden bevatten informatie over de tabellen in de databank. De andere .jsonl-bestanden bevatten de vragen die gesteld kunnen worden aan het systeem, alsook de opbouw van de query's. De uitvoering van de query's zorgen voor een mogelijk antwoord op de beschreven vraag.
	\item[download\_glove.sh] Door de uitvoering van dit script, wordt er een zip-bestand gedownload met vooraf getrainde glove embedding. Dit zip-bestand bevat een tekstbestand, \textit{glove.42B.300d.txt}. Dit bestand heeft een grootte van 4,67 gigabyte.
	\item[extract\_vocab.py] Dit script extraheert de glove embedding van het tekstbestand. Verder zorgt dit script voor de training van de word embedding. 
	\item[requirements.txt] Dit tekstbestand bevat, net zoals bij WikiSQL, alle, via pip install, te installeren modules om de uitvoering van de Python-scripts mogelijk te maken.
	\item[train.py] Dit script dient uitgevoerd te worden om het SQLNet algoritme te trainen. Dit script maakt gebruik van de CUDA-toolkit \footnote{CUDA staat voor Compute Unified Device Architecture. Deze technologie zorgt ervoor dat de programmeur in staat is om algoritmes op de GPU (grafische processor) uit te voeren. Dit mechanisme is ontwikkeld door NVIDIA.} en heeft vereist de aanwezigheid van een NVIDIA-driver \footnote{Oracle Virtual Box maakt gebruik van een eigen, virtuele GPU. Het is niet mogelijk om GPU van het lokale systeem te delen met een virtuele machine. Daarom werd er besloten Ubuntu te installeren op een andere laptop met NVIDIA GPU, zoals vermeld in sectie \ref{sec:opzet} van hoofdstuk \ref{ch:methodologie}.}
	\item[test.py] Dit script zorgt voor de evaluatie van het SQLNet algoritme. Dit op de \textit{dev}- en \textit{test}-dataset. Ook hier wordt gebruik gemaakt van de CUDA-toolkit. Dit algoritme wordt dus ook op de grafische processor (op de grafische kaart) uitgevoerd.
\end{description}

\subsection{Uitvoering}
\label{sec:uitvsqln}

In tegenstelling tot het WikiSQL algoritme, wordt voor de uitvoering van SQLNet gebruik gemaakt van Python-versie 2.7.12. De Python-syntax in de scripts is eigen aan de Python-versie. 

De eerste stap van de uitvoering, is het clonen van de Github-repository naar het lokale systeem van de gebruiker. Wanneer dit voltooid is, is de gebruiker in staat verder te gaan met de installatie van het algoritme. 

Voor de uitvoering van zowel het script voor de training van het algoritme als voor het script voor het evalueren van algoritme is er data nodig. De volgende stap in de installatie van het algoritme is het uitpakken van het tar-bestand waarin de data-bestanden kunnen worden teruggevonden. Via volgend commando wordt dit uitgevoerd:

\begin{center}
	\textit{tar -xvjf data.tar.bz2}
\end{center}

Vervolgens dient men de nodige modules te installeren. Deze modules staan opgelijst in het tekstbestand \textit{requirements.txt}. Alle beschreven modules dienen geïnstalleerd te worden met de beschreven versie. Voor de installatie wordt volgend commando gebruikt:

\begin{center}
	\textit{sudo python -m pip install -r requirements.txt}
\end{center}

Het script stopt tijdens de installatie van module \textit{pkg-resources}, versie 0.0.0. Deze module kan niet geïnstalleerd worden via pip. Volgend commando kan eventueel gebruikt worden:

\begin{center}
	\textit{sudo apt-get install python-pkg-resources}
\end{center}

Er zijn dan nog twee mogelijkheden om de overige modules te installeren. Ofwel kan pkg-resources uit het document gehaald worden en kan het "pip install"-commando opnieuw worden uitgevoerd. Ofwel dient de gebruiker nog alle te installeren modules apart te installeren. Dit kan door middel van volgend commando:

\begin{center}
	\textit{sudo python -m pip install <<module>>}
\end{center}

Vervolgens werd er een shell-script uitgevoerd, \textit{download\_glove.sh}, welke de vooraf getrainde glove embedding download. Dit wordt uitgevoerd door volgend commando uit te voeren:

\begin{center}
	\textit{bash download\_glove.sh}
\end{center}

Bij het uitvoeren van dit script, wordt er eerst gekeken of er een map \textit{glove} bestaat in de map van het algoritme. Zo ja, wordt het glove-bestand naar die map gedownload en uitgepakt. Anders maakt het script eerst een map \textit{glove} aan, wordt het glove-bestand daarin gedownload en uitgepakt. 

Zoals bij alle neurale netwerkalgoritmen, dient dit algoritme ook getraind te worden. Dit gebeurt door \textit{train.py} uit te voeren in het terminal venster. In het overzicht wordt vermeld dat dit script met verschillende parameters kan worden uitgevoerd. Deze parameters worden uitgelegd wanneer men volgend commando uitvoert:

\begin{center}
	\textit{sudo python train.py -h}
\end{center}

Wanneer er een NVIDIA-GPU gebruikt wordt voor deze scripts uit te voeren, geeft het systeem volgende foutmelding terug: \textit{Segmentation fault (core dumped)}. Deze foutmelding kan vermeden worden door gebruik te maken van een andere geïnstalleerde GPU. Voor de interne werking van de scripts dient er een NVIDIA-driver geïnstalleerd te zijn op het systeem. Wanneer het laatst beschreven script is uitgevoerd, worden de parameters uit tabel \ref{table:paramtrain} getoond:

\begin{table}[htb]
	\centering
	\begin{tabular}{ | l | p{5cm} |}
		\hline
		Parameter 			& Uitleg \\ \hline
		-h, --help 			& Toont de helpboodschap voor dit script \\ \hline
		--toy 				& Wordt gebruikt voor snelle debugging \\ \hline
		--suffix SUFFIX		& De suffix op het einde van een saved model naam \\ \hline
		--ca				& Gebruik makend van conditional attention \\ \hline
		--dataset DATASET 	& 0: originele dataset, 1: re-split dataset \\ \hline
		--rl				& Gebruiken van Reïnforcement Learning voor Seq2SQL (vereist voorafgetrainde models) \\ \hline
		--baseline			& Wanneer vermeld wordt Seq2SQL model getraind, standaard wordt het SQLNet model getraind \\ \hline
		--train\_emb		& Het trainen van word embedding voor SQLNet (vereist voorafgetrainde models) \\ \hline	 
	\end{tabular}
	\caption{Parameters train.py}
	\label{table:paramtrain}
\end{table}

Een typische uitvoering van het train-script, is het volgende:

\begin{center}
	\textit{sudo python train.py --ca}
\end{center}

Met de uitvoering van dit script wordt het algoritme getraind met column attention. Naast dit commando worden volgende commando's ook vaak gebruikt voor de training van het algoritme:

\begin{center}
	\textit{sudo python train.py --ca --train\_emb}
\end{center}

Het trainen van het algoritme met column attention en trainbare woordembedding. Hier dient het algoritme wel eerst getraind zonder embedding.

\begin{center}
	\textit{sudo python train.py --baseline --dataset 1}
\end{center}

Het vooraf trainen van een Seq2SQL model. Dit met een re-splitted dataset. 

\begin{center}
	\textit{sudo python train.py --baseline --dataset 1 --rl}
\end{center}

Het trainen van het Seq2SQL model met Reïnforcement Learning, nadat het model al vooraf getraind is.

Tijdens het uitvoeren van de scripts, worden eerst alle data-bestanden opgehaald. Het systeem meld welke bestanden worden ingeladen en welke reeds ingeladen zijn. Nadat alle bestanden uit de data-map ingeladen zijn, wordt uit het bestand uit de glove-map ingeladen. 

Het vervolg van de uitvoering van dit script loopt fout. Het systeem geeft namelijk een \textit{CUDA Runtime Error}. De eerste foutmelding meldt het volgende:

\begin{center}
	\textit{CUDA Runtime Error (35): CUDA Driver version is insufficient for CUDA Runtime version}
\end{center}

Deze foutmelding komt voor wanneer het systeem oudere componenten van het stuurprogramma voor de GPU aanwezig zijn op het systeem. Ook is er een oudere/verkeerde versie van de CUDA-toolkit aanwezig op het systeem. Dit kan verholpen worden door eerst beide stuurprogramma's te verwijderen en deze beide terug te installeren.

Het verwijderen van het NVIDIA-stuurprogramma kan gebeuren met het \textit{sudo apt-get purge}-commando, namelijk \textit{sudo apt-get purge nvidia-*}. Hiermee worden alle reeds geïnstalleerde versies van dit stuurprogramma worden verwijderd. Ook alle configuratiebestanden worden hierdoor verwijderd. Om het NVIDIA-stuurprogramma opnieuw te installeren, kan bijvoorbeeld het \textit{sudo apt-get install nvidia-current}-commando gebruikt worden.

Voor het verwijderen en de herinstallatie van de CUDA-toolkit, werd de blogpost van \textcite{cudainstall} gevolgt. In deze blog wordt beschreven hoe de laatste CUDA-versie kan geïnstalleerd worden voor het besturingssysteem dat gebruikt werd tijdens de experimenten.

Wanneer dit alles geïnstalleerd is, dient het systeem heropgestart te worden. Hier liep het voor de tweede keer fout. Het was enkel mogelijk om Ubuntu te gebruiken vanuit de terminal (bereikbaar door CTRL - ALT - F1 tegelijkertijd ingedrukt te houden). Vanuit deze terminal werd er opnieuw een poging ondernomen voor het uitvoeren van eht trainingsscript, maar opnieuw liep het mis. Nu werd tijdens de uitvoering volgende foutmelding weergegeven:

\begin{center}
	\textit{CUDA Runtime Error (30) unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70}
\end{center}

Op de blog van \textcite{cudablog} werd een mogelijke oplossing gevonden voor dit probleem. Ook werd het onmiddelijk duidelijk dat deze fout niets te maken had met versie probleem bij één van de stuurprogramma's. Volgens de blog kon het probleem opgelost worden door de laptop eens te herstarten. De laptop werd herstart, maar het probleem bleef zich voordoen. Een andere mogelijke oplossing werd niet gevonden. Elke mogelijke oplossing kwam op hetzelfde neer: herstart de computer.

Ook bij de volgende mogelijke uitvoeringen van het script, liep het mis op diezelfde plaats. Altijd kwam dezelfde fout terug. Een gevolg van dit probleem, is het feit dat ook het evaluatiescript, \textit{test.py}, om dezelfde reden niet kon uitgevoerd worden. Dit script zou op dezelfde manier kunnen uitgevoerd worden als het trainingsscript, dus ook met dezelfde parameters.

\subsection{Resultaten}

Het was niet mogelijk om de scripts correct uit te voeren (zie sectie \ref{sec:uitvsqln}). Het gevolg hiervan was dan ook dat er geen resultaten bekomen konden worden. 

\section{Seq2SQL}

\subsection{Algoritme}

Het Seq2SQL algoritme (sectie \ref{sec:Salesforce - Seq2SQL}), welke te verkrijgen is via de Github-repository van \textcite{seq2sql}, biedt de gebruiker een neuraal netwerk aan voor de generatie van query's op basis van natuurlijke taal. Dit algoritme is ontwikkeld door Salesforce, welke ook de WikiSQL dataset heeft ontwikkeld. Seq2SQL wordt getraind op basis van deze laatst beschreven dataset.

In tegenstelling tot SQLNet, maakt Seq2SQL wel gebruik van reïnforcement learning. Het mechanisme krijgt een ssort belong wanneer er een goede query wordt gegenereerd. 

\subsection{Opbouw}

De complete Github-repository van Seq2SQL (\textcite{seq2sql}) bestaat grotendeels uit uit te voeren bash-scripts. Deze scripts worden gebruikt voor de uitvoering van het evaluatie script. Naast scripts voor de uitvoering van het algoritme, wordt er ook een script voorzien voor de installatie van het model (\textit{install-g++.sh}).

Ook worden er zes mappen vermeld in het overzicht van de repository:

\begin{description}
	\item[data] Deze map bevat op zijn beurt ook vijf mappen, elk met .tsv-bestanden die de data bevat, nodig voor de uitvoering van de scripts. Ook wordt hier WikiSQL-data in teruggevonden. De reden hiervoor is, omdat zoals ook al eerder vemeld, het SeqSQL algoritme getraind wordt op basis van deze dataset.
	\item[evaluator] Deze map bevat programma's en scripts die kunnen gebruit worden voor de evaluatie van eht algoritme. De meeste scripts zijn geschreven in Python en Scala. Maar er kunnen ook programma's en scripts teruggevonden worden in Java. 
	\item[fig] Deze map bevat afbeeldingen van grafieken waarbij de nauwkeurigheid van het algoritme wordt in aangetoond. Er worden drie verschillende grafieken getoond, onder andere die inzake het sequence-to-sequence model als dat over tokenizatie.
	\item[lib] Deze map bevat scripts en programma's die nodig zijn voor het uitvoeren van het trainings- en evaluatiescript. Hier vinden we onder andere het script zodat het model toegang krijgt tot de databank.
	\item[sqlnet] Deze map bevat een link naar de Github-repository van het SQLNet algoritme. Zowel het Seq2SQL algoritme als het SQLNet algoritme zijn op een gelijkaardige manier opgebouwd. Enkel gebruikt Seq2SQL reïnforcement learning, terwijl SQLNet dit niet toepast.
	\item[src] Deze map bevat een groot aantal scripts en programma's die gebruikt worden tijdens de uitvoering van het script. Het belangrijkste script is \textit{main.py}. Dit script wordt tijdens de uitvoering van elk bash-script uitgevoerd.
\end{description}

\subsection{Uitvoering}

Er wordt geen \textit{requirements.txt}-bestand voorhanden gegeven in de Github-repository. Op voorhand modules installeren die eventueel nodig waren voor de uitvoering van de scripts lukte dus niet. 

Het eerste bash-script dat werd uitgevoerd was \textit{install-g++.sh}. Dit werd als volgt uitgevoerd:

\begin{center}
	\textit{bash install-g++.sh}
\end{center}

Dit script dient voor de installatie van \textit{g++}, een compiler-stuurprogramma van de GNU\footnote{GNU is een besturingssysteem welke ontwikkeld is met vrije software, dus de vrijheid van de gebruiker wordt gerespecteerd (zie ook \textcite{gnu}).} Compiler Collection. \textit{gcc} is ook zo een compiler. \textit{g++} is de GNU C++-compiler, terwijl \textit{gcc} de GNU C-compiler is.

De uitvoering van het script loopt echter fout. Het script meld dat een bepaalde map niet bestaat op het systeem. Er werd gecontroleerd of er reeds een \textit{g++}-compiler geïnstalleerd stond op het systeem, via het "\textit{g++ --version}"-commando. Dit bleek het geval te zijn. Ook was er reeds een versie van de \textit{gcc}-compiler geïnstalleerd.

Vervolgens werd het \textit{scratch.py}-script uitgevoerd. Dit gebeurde via volgend commando:

\begin{center}
	\textit{sudo python scratch.py}
\end{center}

Hier werd eerst een melding gegeven dat de module \textit{theano} niet kon gevonden worden. Deze module diende nog geïnstalleerd te worden:

\begin{center}
	\textit{sudo python -m pip install theano}
\end{center}

Het scratch-script kon opnieuw uitgevoerd worden. Indien er gemeld zou worden dat de module \textit{numpy} niet zou bestaan, kan deze op een gelijkaardige manier geïnstalleerd worden.

Na de uitvoering wordt het volgende bekomen: \\
(1, 5, 3) \\
(3,) \\
(1, 5)

Tijdens de uitvoering van dit script eerst een array aangemaakt door middel van de \textit{numpy}-module. Op deze array wordt de \textit{expands\_dims}-methode uitgevoerd, waarbij de array uitgebreid wordt door een nieuwe as op de opgegeven positie in te voegen. De \textit{shape}-methode schrijft de dimensie vaan de array uit. 

Vervolgens kan ieder bash-script uitgevoerd worden. Er werd gestart met \textit{atis.sh}, waarbij volgend commando gebruikt werd voor het starten:

\begin{center}
	\textit{sudo bash atis.sh}
\end{center}

Tijdens de uitvoering werd wel gemeld dat de module \textit{dill} dient geïnstalleerd te worden. Dit kan ook via het \textit{pip install}-commando, namelijk:

\begin{center}
	\textit{sudo python -m pip install dill}
\end{center}

Verder dient de modules \textit{tqdm} en \textit{MySQLdb} geïnstalleerd te worden. \textit{tqdm} kan ook via \textit{pip install} geïnstalleerd worden, terwijl \textit{MySQLdb} op volgende manier kan geïnstalleerd worden:

\begin{center}
	\textit{sudo apt-get install python-mysqldb}
\end{center}

Nadat alle nodige modules geïnstalleerd waren, werd het bash-script opnieuw gestart. Intern werd het \textit{main.py} script uitgevoerd met een aantal parameters. In tabel \ref{table:parammain} worden deze parameters beschreven.

\begin{table}[]
	\centering
	\begin{tabular}{ | l | p{10cm} |}
		\hline
		Parameter 			& Uitleg \\ \hline
		-d 					& Dimensie van de verborgen eenheden \\ \hline
		-i 					& Dimensie van input vectoren \\ \hline
		-o					& Dimensie van output woordvectoren \\ \hline
		-p					& Manier van het kopiëren van woorden, mogelijkheden: none, attention, attention-logic \\ \hline
		-u 					& Behandel inputwoorden met <= wanneer deze veel voorkomen als \textit{unknown (UNK)} \\ \hline
		-t					& Hoeveel perioden het model getraind wordt (standaard is geen training) \\ \hline
		-c					& Type van continuous recurrent neuraal netwerk (RNN), mogelijkheden: vanillarnn, gru, lstm, atnh \\ \hline
		-m					& Algemeen model type, mogelijkheden: encoderdecoder, attention, attn2hist \\ \hline	 
		--stats-file		& Het pad waar de statistieken wordden opgeslagen als JSON-bestanden \\ \hline	 
		--domain			& Domein waarop getest dient te worden \\ \hline	 
		-k					& Gebruik bundel zoeken met gegeven bundelgrootte \\ \hline	 
		--dev-seed			& Willekeurige nummergenerator (RNG) voor de training/dev splits (standaard = 0) \\ \hline	 
		--model-seed		& Willekeurige nummergenerator (RNG) voor de model initialisatie en SGD ordening (standaard = 0) \\ \hline	 
		--train-data		& Het pad naar de trainingsdata \\ \hline	 
		--dev-data			& Het pad naar de developer data \\ \hline	 
		--train-source-file	& Het bronbestand nodig voor voorspelling \\ \hline	 
		--train-db-file		& De brondatabank nodig voor voorspelling \\ \hline	 
		--train-table-file	& Bestand met trainingstabellen \\ \hline	 
	\end{tabular}
	\caption{Parameters main.py}
	\label{table:parammain}
\end{table}

De uitvoering van het script startte met de initialisatie van het \textit{main.py}-script. Alle parameters worden uitgelezen en toegevoegd aan dat Python-script. Vervolgens wordt de SQL-vocabulair onttrokken. Verder wordt in deze setup alles klaargemaakt om de uitvoering mogelijk te maken. Hier werden echter foutmeldingen gegeven. Het systeem gooit een TypeError. Dit gebeurt een aantal keer. Uiteindelijk meld het systeem dat de setup compleet is en start de uiteindelijke uitvoering.

Tijdens de uitvoering van het script werden de vragen, opgelijst in de tsv-databestanden voor dat script (wat meegegeven werd als parameter bij \textit{--train-data}), overlopen. Hierbij werd voor elke vraag een query getest op de opgegeven dataset. In de terminal werd elke vraag weergegeven, als de bijhorende query. Verder werd een array weergegeven met de kansen dat met de uitvoering van deze query het juiste resultaat zou bekomen worden.

De uitvoering van dit script werd een aantal keren herhaald. Zo werden de vragen en query's na meer dan vierentwintig uur nog steeds doorlopen. Het grote voordeel van dat de uitvoering zich continu herhaald, is het feit dat het duidelijk wordt dat de query's zich altijd trachten te verbeteren. Per iteratie werden de kansen in de array aangepast.

Aangezien de uitvoering zo'n lange tijd in beslag nam, werd er besloten de andere scripts ook te starten. De werking van deze scripts was natuurlijk gelijk aan de werking van het \textit{atis}-script. Ook hier werd per vraag de bijhorende query uitgevoerd op de bijhorende dataset en worden de kansen ook in een array weergegeven.

Het \textit{atis}-script werd gestopt na meer dan 48 uren doorlooptijd. De andere scripts werden gestopt na meer dan vierentwintig uur. Het algoritme bleef alle controles opnieuw uitvoeren en alles opnieuw oplijsten. Zoals reeds eerder vermeld, werd het duidelijk dat het model zich steeds trachtte te verbeteren. De kans op slagen groeide per iteratie. Dit heeft natuurlijk te maken het reïnforcement learning. Het systeem tracht zich altijd te verbeteren, door per iteratie te vertrekken vanuit de best uitgevoerde query. 

\subsection{Resultaten}

De uitvoering van sommige scripts duurden meer dan 48 uur. Er werd niet concreet gemeld wanneer de scripts konden gestopt worden, of het weldegelijk mogelijk was om de scripts te stoppen. Verder werden geen enkele scripts voorzien voor het testen en evalueren van het model. Er werd getracht de huidige scripts wat aan te passen, maar de uitvoering bleef maar duren. Ook de scripts, vermeld in de evaluator map, bekomen niet de gewenste resultaten. Er werden dus geen duidelijke resultaten verkregen, alhoewel dat het na een aantal iteraties duidelijk werd dat dit algoritme gebruik maakte van reïnforcement learning. Het algoritme tracht zichzelf iedere iteratie te verbeteren.

\section{Algemeen}

Gezien de onderzochte scripts niet optimaal werkten en er geen werkende oplossingen voorhanden waren, was het ook niet mogelijk deze scripts uit te testen op een andere dataset. Er kon geen duidelijke vergelijking gemaakt worden tussen de originele uitvoering en de uitvoering met de nieuwe dataset. Enkel het laatst beschreven algoritme, \textit{Seq2SQL}, werd uitgetest op meerdere datasets, maar de uitvoering van de scripts gaven geen duidelijke resultaten.

\begin{table}[]
	\centering
	\begin{tabular}{ | l | l | p{5cm} | p{5cm} |}
		\hline
		Dataset & Na \dots procent & Query & Uitvoering in SQLite-browser \\ \hline
		train 	& 41 			   & SELECT col4 FROM table WHERE col0 < 1,236 AND col2 = \$817,781 & table\_2\_13663314\_1, resultaat: results  \\ \hline
		train 	& 42 			   & SELECT col4 FROM table WHERE col1 = civil parish AND col3 = Copeland AND col2 < 1,280 AND col0 = Ennerdale and kinnisde  & table\_2\_1401800\_1, resultaat: ennerdale rural district  \\ \hline
		train 	& 43 			   & SELECT col0 FROM table WHERE col2 < 11,484 & table\_2\_1601940\_1, resultaat: labour  \\ \hline
		train 	& 54 			   & SELECT col0 FROM table WHERE col5 < 4,485 & table\_2\_16653153\_28, resultaat: [4 february, 6 february, 7 february, 7 february]  \\ \hline
		train 	& 57 			   & SELECT col3 FROM table WHERE col6 < 1,258.1 AND col5 < 11.6 & table\_2\_1682026\_3, resultaat: insurance  \\ \hline
		train 	& 58 			   & SELECT col3 FROM table WHERE col5 = sunk (mine) AND col4 < 2,266 & table\_2\_17166347\_1, resultaat: uk  \\ \hline
		train 	& 59 			   & SELECT col2 FROM table WHERE col1 = 102.3 AND col3 < 1,000 & table\_2\_17071097\_1, resultaat: kingman, arizona  \\ \hline
		train 	& 61 			   & SELECT col2 FROM table WHERE col3 < 1,575.0 AND col4 = 1997 AND col0 = philae sulcus & table\_2\_16768245\_5, resultaat: 169.0w  \\ \hline
		train 	& 65 			   & SELECT col2 FROM table WHERE col4 > 94 AND col1 < 5,297 AND col3 > 0.6106 & table\_2\_11513625\_11, resultaat: 54.0  \\ \hline
		train 	& 80 			   & SELECT col0 FROM table WHERE col3 < 1,429 & table\_2\_176529\_1, resultaat: [dorchester, port elgin]  \\ \hline
		train 	& 80 			   & SELECT col3 FROM table WHERE col5 < 1,000 AND col4 = +9 & table\_2\_18007119\_1, resultaat: 70-76-75-76=297  \\ \hline
		train 	& 85 			   & SELECT col1 FROM table WHERE col3 < 5,361 AND col5 < 4,841 AND col4 < 618 AND col2 < 389 & table\_2\_18894903\_1, resultaat: [91, 907, n, n, n, n, 773, \dots] (16 rijen in totaal)  \\ \hline
		train	& 89 			   & SELECT col0 FROM table WHERE col2 < 2,198.72 AND col1 > 90909 & table\_2\_1894556\_1, resultaat: [bochum part 1, ramutla, seshego]  \\ \hline
		train	& 89 			   & SELECT col4 FROM table WHERE col1 < 8,085 AND col3 < 145 & table\_2\_189893\_1, resultaat: 0.42\%  \\ \hline
	\end{tabular}
	\caption{Overzicht misgelopen query's WikiSQL - deel train}
	\label{table:wikisqlerrorstrain}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{ | l | l | p{5cm} | p{5cm} |}
		\hline
		Dataset & Na \dots procent & Query & Uitvoering in SQLite-browser \\ \hline
		dev 	& 30 			   & SELECT col0 FROM table WHERE col5 < 1,000 & table\_2\_10581768\_2, resultaat: [alice lloyd college, brescia university]  \\ \hline
		dev 	& 43 			   & SELECT col6 FROM table WHERE col1 < 1,352.1 AND col4 > 21,615 AND col5 < 75.4 AND col7 = 2.9  & table\_2\_1598533\_8, resultaat: -0.6  \\ \hline
		dev 	& 52 			   & SELECT col0 FROM table WHERE col3 < 87,585 AND col2 < 1.28 AND col1 > 27504 AND col4 = xhosa & table\_2\_1720632\_2, resultaat: [khaya mnandi, kwa langa, woodridge]  \\ \hline
		dev 	& 59 			   & SELECT col0 FROM table WHERE col5 < 3,000 AND col1 = marty furgol & table\_2\_17290169\_4, resultaat: t9  \\ \hline
		dev 	& 84 			   & SELECT col4 FROM table WHERE col3 = north carolina AND col1 < 21,500 & table\_2\_18204624\_1, resultaat: 1926  \\ \hline
	\end{tabular}
	\caption{Overzicht misgelopen query's WikiSQL - deel dev}
	\label{table:wikisqlerrorsdev}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{ | l | l | p{5cm} | p{5cm} |}
		\hline
		Dataset & Na \dots procent & Query & Uitvoering in SQLite-browser \\ \hline
		test 	& 38 			   & SELECT col1 FROM table WHERE col3 < 100,000 AND col2 = guymon, oklahoma & table\_2\_14203256\_1, resultaat: 91.3 fm  \\ \hline
		test 	& 58 			   & Query SELECT col1 FROM table WHERE col3 < 1,137 AND col0 = pointe-verte & table\_2\_171222\_1, resultaat: village  \\ \hline
		test 	& 95 			   & Query SELECT col2 FROM table WHERE col3 < 32,000 AND col1 = requebra & table\_2\_12937449\_4, resultaat: no. 8  \\ \hline
	\end{tabular}
	\caption{Overzicht misgelopen query's WikiSQL - deel test}
	\label{table:wikisqlerrorstest}
\end{table}